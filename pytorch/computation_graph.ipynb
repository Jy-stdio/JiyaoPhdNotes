{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图构建细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "+ https://github.com/chunhuizhang/bilibili_vlogs/blob/master/learn_torch/grad/05_torch_variables_grad_inplace_operation.ipynb\n",
    "+ https://www.bilibili.com/video/BV1o24y1b7tk?vd_source=225dba48b31d269151658db856705273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inplace operation介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，导入一些package，查看当前Python解释器的版本信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "sys.version_info(major=3, minor=8, micro=17, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "print(torch.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的训练流程为：\n",
    "\n",
    "1. compute loss: forward\n",
    "2. loss.backward(): (compute grad)\n",
    "3. optimizer.step(): $x = x - lr \\cdot x.grad$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们分别介绍两种不被允许的inplace operation\n",
    "\n",
    "1. 对于`requires_grad==True`叶子张量不能使用inplace operation\n",
    "+ all `Parameters` are leaf nodes and requirs grad.\n",
    "+ 使用tensor.is_leaf 判断是否为叶子结点.\n",
    "\n",
    "2. 对于在求梯度阶段用到的张量不能使用inplace operation.\n",
    "\n",
    "---\n",
    "\n",
    "下面举两个例子说明什么是inplace operation以及如何规避这种情况：\n",
    "\n",
    "1. 对于`requires_grad==True`叶子张量不能使用inplace operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.FloatTensor(10)\n",
    "w.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf # w is a leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用normal_操作（带`_`的操作一般为inplace operation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb 单元格 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# inplace operation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m w\u001b[39m.\u001b[39;49mnormal_()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# inplace operation\n",
    "w.normal_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到直接对叶子节点做inplace operation会报错，那么如果要修改叶子节点，如何避免这种问题呢？可以对`.data`执行inplace operation："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.7905, -1.3713,  0.0745, -0.1921,  1.1473,  0.1278,  0.9638,  1.6434,\n",
       "        -0.8331,  0.5344])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(w.data.requires_grad)\n",
    "print(w.data)\n",
    "w.data.normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7905, -1.3713,  0.0745, -0.1921,  1.1473,  0.1278,  0.9638,  1.6434,\n",
       "        -0.8331,  0.5344])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 求梯度阶段（不限于是否是leaf node/variable/Parameters）需要用到的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有如下计算图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1., 2.])\n",
    "w1 = torch.FloatTensor([[2,],[1.]])\n",
    "w2 = torch.FloatTensor([3.])\n",
    "w1.requires_grad = True\n",
    "w2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们执行inplace operation：\n",
    "\n",
    "> 代码会发生报错，原因是：\n",
    "> + 在计算f的时候，d是等于某个值的，f对于w2的导数适合d值相关的。但是在计算完f之后，d的值被修改，这会倒是f.backward()对于w2的导数计算错误。具体来说，在执行f = torch.matmul(d, w2)这句的时候，pytorch的反向求导机制保存了d的引用，为了之后的反向求导计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]], which is output 0 of torch::autograd::CopySlices, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb 单元格 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(d, w2)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m d[:] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m f\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]], which is output 0 of torch::autograd::CopySlices, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# x * w1 -> d; d * w2 -> f\n",
    "d = torch.matmul(x, w1)\n",
    "f = torch.matmul(d, w2)\n",
    "d[:] = 0\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们在计算f = torch.matmul(d, w2)之前修改d，就不会出现上述问题：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x * w1 -> d; d * w2 -> f\n",
    "d = torch.matmul(x, w1)\n",
    "d[:] = 0\n",
    "f = torch.matmul(d, w2)\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.data`和`.detach`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二者的联系与区别，以及需要注意的问题：\n",
    "+ `detach`\n",
    "    - Returns a new Tensor, detached from the current graph.\n",
    "    - The result will never require gradient.\n",
    "+ `x.data` 与 `x.detach()` 返回的 tensor 有相同的地方, 也有不同的地方，相同点如下:\n",
    "    - 都和 x 共享同一块数据\n",
    "    - 都和 x 的 计算历史无关\n",
    "    - `requires_grad = False`\n",
    "不同点如下\n",
    "    - x.data 的修改不会导致报错，但其实计算是有问题的（相当于埋了一个bug）；\n",
    "    - x.detach() 会直接报错（**更加梯度安全，推荐使用**）；\n",
    "    \n",
    "下面是分别使用`.data`和`.detach()`的两个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad: True, out.requires_grad: True, c.requires_grad: False\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "tensor([0., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0.]) tensor([0.1966, 0.1050, 0.0452], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "\n",
    "out = a.sigmoid()\n",
    "\n",
    "c = out.data\n",
    "# c = out.detach()\n",
    "\n",
    "print(f'a.requires_grad: {a.requires_grad}, out.requires_grad: {out.requires_grad}, c.requires_grad: {c.requires_grad}')\n",
    "\n",
    "print(out)\n",
    "print(c)\n",
    "c.zero_()\n",
    "\n",
    "print(out)\n",
    "print(c)\n",
    "\n",
    "out.sum().backward()\n",
    "print(a.grad, a.sigmoid()*(1-a.sigmoid()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad: True, out.requires_grad: True, c.requires_grad: False\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "tensor([0., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0., 0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb 单元格 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(c)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m out\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mgrad, a\u001b[39m.\u001b[39msigmoid()\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39ma\u001b[39m.\u001b[39msigmoid()))\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "\n",
    "out = a.sigmoid()\n",
    "\n",
    "# c = out.data\n",
    "c = out.detach()  # 更安全，当执行inplace operation的时候会报错\n",
    "\n",
    "print(f'a.requires_grad: {a.requires_grad}, out.requires_grad: {out.requires_grad}, c.requires_grad: {c.requires_grad}')\n",
    "\n",
    "print(out)\n",
    "print(c)\n",
    "c.zero_()\n",
    "\n",
    "print(out)\n",
    "print(c)\n",
    "\n",
    "out.sum().backward()\n",
    "print(a.grad, a.sigmoid()*(1-a.sigmoid()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "我们最后介绍一个Embedding例子，说明inplace operation问题。一定要注意构建图的顺序关系！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在计算b的时候，max_norm的操作使得embedding.weight发生改变，产生与a的计算使用的weight产生冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 5]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb 单元格 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m out \u001b[39m=\u001b[39m (a\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m b\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msigmoid()\u001b[39m.\u001b[39mprod()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liujiyao/Documents/PhdLearningNotes/JiyaoPhdNotes/pytorch/computation_graph.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torchcpu/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 5]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "n, d, m = 3, 5, 7\n",
    "# embedding = nn.Embedding(n, d, max_norm=True)\n",
    "embedding = nn.Embedding(n, d, max_norm=1)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight @ W.t()  # weight must be cloned for this to be differentiable \n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对weight进行clone操作，使得之后weight的改变不会影响a的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d, m = 3, 5, 7\n",
    "# embedding = nn.Embedding(n, d, max_norm=True)\n",
    "embedding = nn.Embedding(n, d, max_norm=1)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # TODO: (done) weight must be cloned for this to be differentiable \n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
