#! https://zhuanlan.zhihu.com/p/681394812
# Visual ML | Statistics | 20.贝叶斯推断入门

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/lzwmQR.png)

![贝叶斯推断知识图](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204150128737.png)

## 贝叶斯推断定义

贝叶斯推断的核心是在以往的经验 (先验概率) 基础上，结合新的数据，得到新的概率 (后验概 率)。而模型参数分布随着外部样本数据不断输入而迭代更新。不同的是，**频率派只考虑样本数据本身，不考虑先验概率。**

![通过贝叶斯定理迭代学习](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204150730958.png)

贝叶斯推断定义为
$$
\underbrace{f_{\Theta\mid X}(\theta\mid x)}_{后牙}=\frac{\underbrace{f_{X\mid\Theta}(x\mid\theta)}_{似然}\underbrace{f_{\Theta}(\theta)}_{先验}}{\underbrace{\underset{\vartheta}{\operatorname*{\int}}f_{X\mid\Theta}(x\mid\theta)f_{\Theta}(\theta)\mathrm{d}\theta}_{证据}}
$$
在上式中未知参数$\Theta$为随机变量，$\theta$为一个随机变量的取值。当$\Theta=\theta$取不同值时，**似然**分布也有相应的变化。**似然×先验**表示$X$和$\Theta$的**联合分布**，**证据**为该联合分布的边缘分布，与$\theta$无关，意味着数据对先验的选择没有影响。后验代表在整合“先验 + 样本数据”之后，我们对参数$\Theta$ 的新的“认识”。在连续迭代贝叶斯学习中，这个后验概率分布是下一个迭代的先验概率分布。

为了的到真正的后验概率密度，我们需要求得证据的积分。但是这个积分一般来说没有闭式解（一般来说仅**共轭先验可以求得闭式解，即后验和先验都是同类分布**），因此可能需要通过数值积分或蒙特卡洛模拟。

## 从鸡兔同笼问题出发，研究贝叶斯推断

> 问题：一个巨大无比农场散养大量“走地”鸡、兔。但是，农夫自己也说不清楚鸡兔的比例。 用$\Theta$代表兔子的比例随机变量，这意味着$\Theta$的取值范围为 [0, 1]。为了搞清楚农场鸡兔比例，农夫决定随机抓$n$只动物。$X_1,X_2\ldots X_n$为每次抓取动物的结果。 $X_i$的样本空间为$\{0,1\}$，其中 0 代表鸡，1 代表兔。

根据农夫对农场的大致了解，我们从如下几个角度分别分析不同先验得到的不同后验结果：①农夫完全不确定鸡兔的比例，我们选择连续均匀分布 Uniform(0, 1) 为先验分布；②设农夫认为鸡兔的比例接近 1:1，农夫对这个比例的确信程度不同，先验分布为$Bata(\alpha, \alpha)$；③更一般的情况：并不确定具体比例接近几比几，先验分布为$Beta(\alpha, \beta)$分布。

### 1.农夫完全不确定鸡兔比例，先验分布为均匀分布

**理论分析**

由于农夫完全不确定鸡兔的比例，我们选择连续均匀分布 Uniform(0, 1) 为先验分布：$f_\Theta\left(\theta\right)=1,\quad\theta\in[0,1]$，称为**无信息先验**。在给定$\Theta=\theta$条件下，观测样本$X_i=x_i$（代表某一次抓到的动物，0 代表鸡，1 代表兔）服从IID的Bernoulli(*θ*)：$\underbrace{f_{X_i\mid\Theta}\left(x_i\mid\theta\right)}_{\mathrm{Likelihood}}=\theta^{x_i}\left(1-\theta\right)^{1-x_i}$。

则联合分布为
$$
\begin{aligned}
\underbrace{f_{X_1,X_2,...,X_n,\Theta}\left(x_1,x_2,...,x_n,\theta\right)}_{\mathrm{Joint}}& =\underbrace{f_{X_1,X_2,\ldots,X_n|\Theta}\left(x_1,x_2,\ldots,x_n\mid\theta\right)}_{\text{Likelihood}}\underbrace{f_\Theta\left(\theta\right)}_{\text{Prior}}  \\
&=f_{X_1|\Theta}\left(x_1\mid\theta\right)\cdotp f_{X_2|\Theta}\left(x_2\mid\theta\right)\cdotp\cdotp\cdotp f_{X_n|\Theta}\left(x_n\mid\theta\right)\cdotp\underbrace{f_{\Theta}\left(\theta\right)}_{\mathrm{l}} \\
&=\prod_{i=1}^n\theta^{x_i}\left(1-\theta\right)^{1-x_i}=\theta^{\sum_{i=1}^nx_i}\left(1-\theta\right)^{n-\sum_{i=1}^nx_i}\\
&=\theta^s(1-\theta)^{n-s}, s = \sum_{i=1}^nx_i（n次抽取中兔子抽中的次数）
\end{aligned}
$$
证据为上述联合分布的积分，该积分恰好为Beta函数：
$$
\begin{aligned}
f_{X_1,|X_2,...,X_n}\left(x_1,x_2,...,x_n\right)& =\int_\theta f_{X_1,X_2,...,X_n,\Theta}\left(x_1,x_2,...,x_n,\theta\right)\mathrm{d}\theta   \\
&=\int_\theta\theta^s\left(1-\theta\right)^{n-s}\operatorname{d}\theta \\
&=\int_\theta\theta^{s+1-1}\left(1-\theta\right)^{n-s+1-1}\operatorname{d}\theta\\&=\operatorname{B}{\left(s+1,n-s+1\right)}=\frac{s!\left(n-s\right)!}{\left(n+1\right)!}
\end{aligned}
$$
由此，在$X_1=x_1,X_2=x_2,\ldots,X_n=x_n$条件下，$\Theta$的后验分布为：
$$
 \begin{aligned}
f_{\Theta\mid X_1,X_2,...,X_n}\left(\theta\mid x_1,x_2,...,x_n\right)& =\frac{\overbrace{f_{X_1,X_2,\ldots,X_n,\Theta}\left(x_1,x_2,\ldots,x_n,\theta\right)}^{\text{Joint}}}{\underbrace{f _ { X _ 1 , X _ 2 , \ldots , X _ n }\left(x_1,x_2,\ldots,x_n\right)}_{\text{Evidence}}}  \\
&=\frac{\theta^s\left(1-\theta\right)^{n-s}}{\mathrm{B}\left(s+1,n-s+1\right)}=\frac{\theta^{\left(s+1\right)-1}\left(1-\theta\right)^{\left(n-s+1\right)-1}}{\mathrm{B}\left(s+1,n-s+1\right)}
\end{aligned}
$$
上式中分母的作用仅仅是条件概率归一化。而该后验分布，恰好为**Beta(*s* + 1, *n* – *s* + 1)分布**！

![先验 U(0, 1) + 样本 (*s*, *n* – *s*) → 后验 Beta(*s* + 1, *n* – *s* + 1)](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204160332952.png)

**模拟实验**

实际上 Uniform(0, 1) 就是 Beta(1, 1)。我们可以把蒙特卡洛模拟代入后验Beta(*s* + 1, *n* – *s* + 1)，得到后验分布的形状。

>  说明：本文使用伯努利随机数发生器模拟抽样过程，设置真实兔子占比为0.45。

模拟实验结果如下图所示：图a))为200次采样，每次采样的结果（鸡0or兔1）；图b)表示随着纵坐标采样总次数的增多，鸡和兔的真实比例的变化；图c)表示后验概率随着纵坐标样本数量的变化。

![试验的模拟结果，先验分布为 Beta(1, 1)](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204160630523.png)

下图为后验分布随着样本数的变化：灰色为真实兔子的比例

![九张不同节点的后验概率分布曲线快照，先验分布为 Beta(1, 1)。可以看出信念越来越强](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204163043426.png)

**实验结论如下：**

1. 后验概率分布 $f_{\theta|X}(\theta|x)$曲线不断变的细高，也就是后验标准差不断变小。这是因为样本数据不断增多，大家对鸡兔比例变得越发“确信”。
2. 上图中黑色划线为农场兔子的真实比例；各个子图中红色划线对应的就是后验概率分布的最大值，**最大后验**在逼近兔子的真实比例。
3. 上述贝叶斯推断可以看做最大后验（MAP），其和频率派极大似然（MLE）的区别在于：**MAP可以融合主观（先验）信息！**（虽然此处我们提供的是无信息先验，后文可以我们通过提供其他先验作为例子），可以证明的是：**贝叶斯推断在客观数据不足的时候最好用（适应于样本较小），当样本量非常大时，频率派更有用。**
4. 先验分布的选择和参数的确定代表“经验”，也代表某种“信念”。先验分布的选择和样本数据无关，不需要通过样本数据构造。反过来，观测到的样本数据对先验的选择没有任何影响。贝叶斯推断可以采用迭代方式，即后验概率可以成为新样本数据的先验概率。

### 2. 农夫认为鸡兔的比例接近 1:1，但是对这个比例的确信程度不同，先验分布为$Bata(\alpha, \alpha)$

由于农夫认为鸡兔的比例为 1:1，也就是先验的均值接近0.5。因此，我们选用$\mathrm{Beta}(\alpha,\alpha)$作为先验分布：
$$
f_\Theta(\theta)=\frac1{\mathrm{B}(\alpha,\alpha)}\theta^{\alpha-1}\left(1-\theta\right)^{\alpha-1}, \mathrm{B}(\alpha,\alpha)=\frac{\Gamma(\alpha)\Gamma(\alpha)}{\Gamma(\alpha+\alpha)}
$$
根据确信程度不同，选取不同的$\alpha$，我们可以得到不同的先验：

![五个不同参数 *α* 取不同值时 Beta(*α*, *α*) 分布 PDF 图像](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204164235823.png)

之后的具体推导和前文类似，可以直接看参考材料中的原文，在这里我们只看一下不同先验$\alpha$下的实验结果：

①下图为$\alpha=2$的实验结果，也就是对鸡兔比例为1:1的确信度不高。

![九张不同节点的后验概率分布曲线快照，先验分布为 Beta(2, 2)](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204165130774.png)

② $\alpha=16$，确信度很高

![九张不同节点的后验概率分布曲线快照，先验分布为 Beta(16, 16)](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204165153389.png)

结论：先验分布 Beta(*α*, *α*) 中 *α* 越大，代表主观经验越发“先入为主”，对贝叶斯推断最终结果越强。表现在图 25 中就是，随着 *α* 增大，似然分布和后验分布差异越大，MAP 优化解越发偏离 MLE 优化解，如下图：

![. 对比先验分布、似然分布、后验分布，*α* 取不同值时](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204165333677.png)

### 3.并不确定具体比例接近几比几，先验分布为$Beta(\alpha, \beta)$

并不确定具体比例接近几比几，先验分布为$Beta(\alpha, \beta)$。所示一组图像代表比例和确信度同时变化：

![. 比例和确信程度同时变化](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240204165711625.png)

推导的到后验的形式如下：
$$
\begin{aligned}
f_{\Theta\mid X_1,X_2,...,X_n}\left(\theta\mid x_1,x_2,...,x_n\right)& =\frac{f_{X_1,X_2,...,X_n,\Theta}\left(x_1,x_2,...,x_n,\theta\right)}{f_{X_1,X_2,...,X_n}\left(x_1,x_2,...,x_n\right)}  \\
&=\frac{\frac1{\mathrm{B}\left(\alpha,\beta\right)}\theta^{s+\alpha-1}\left(1-\theta\right)^{n-s+\beta-1}}{\underline{\mathrm{B}\left(s+\alpha,n-s+\beta\right)}}=\frac{\theta^{s+\alpha-1}\left(1-\theta\right)^{n-s+\beta-1}}{\mathrm{B}\left(s+\alpha,n-s+\beta\right)}
\end{aligned}
$$
这里就不做具体的可视化了。

## 贝叶斯收缩

先验$\mathrm{Beta}(\alpha,\beta)$的后验推导的结果为：$\mathrm{Beta}(s+\alpha,n-s+\beta)$，后验的众数（均值）为$\frac{s+\alpha-1}{n+\alpha+\beta-2}$。定义$w=\frac{\alpha+\beta-2}{n+\alpha+\beta-2}, 1-w=\frac n{n+\alpha+\beta-2}$可以把该式写成两部分：
$$
\begin{aligned}\frac{s+\alpha-1}{n+\alpha|+\beta-2}&=w\times\underbrace{\frac{\alpha-1}{\alpha+\beta-2}}_{\text{Prior mode/mean}}+\left(1-w\right)\times\underbrace{\frac sn}_{\text{Sample mean}}\end{aligned}
$$
随着样本量$n$增大，$w$趋向于0，$1-w$趋向于1，也就是说**！！随着样本数据量不断增多，先验的影响力不断减小。$n$趋近无穷时，MAP和MLE结果趋同。**这证明了前文的结论：**贝叶斯推断在客观数据不足的时候最好用（适应于样本较小），当样本量非常大时，频率派更有用。**

**！！当$n$较小时候，先验影响力很大，MAP的结果向先验均值收缩，这种效果称为贝叶斯收缩(Bayes shrinkage)。****当然，*α* 和 *β* 越大，先验的“主观”影响力也会越大。**

换个角度，当我们对参数先验知识模糊不清时，Beta(1, 1) 并非唯一选择。任何 *α* 和 *β* 较小的Beta 分布都可以。因为随着样本数量不断增大，先验分布的较小参数对后验影响微乎其微。

## 总结

贝叶斯推断所体现出来的“学习过程”和人类认知过程极为相似，贝叶斯推断的优点在于其能够利用**先验信息**和后验概率，通过不断更新来获得更准确的估计结果。总结来说，贝叶斯推断的过程包括以下几个步骤：**1) 确定模型和参数空间，建立参数的先验分布；2) 收集数据；3) 根据样本数据，计算似然函数；4) 利用贝叶斯定理，将似然函数与先验概率相结合，计算后验概率；5) 根据后验概率，更新先验概率，得到更准确的参数估计。**

当然，本节只是最简单的贝叶斯推断，志在了解贝叶斯推断背后的思想。希望大家能通过本章例子理解贝叶斯推断背后的思想，以及整条技术路线。



## 参考

[1] [20.贝叶斯推断入门.pdf](https://github.com/Visualize-ML/Book5_Essentials-of-Probability-and-Statistics/blob/main/Book5_Ch20_%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD%E5%85%A5%E9%97%A8__%E7%BB%9F%E8%AE%A1%E8%87%B3%E7%AE%80__%E9%B8%A2%E5%B0%BE%E8%8A%B1%E4%B9%A6__%E4%BB%8E%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%88%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.pdf)

[2] 文中demo实验代码：https://github.com/Visualize-ML/Book5_Essentials-of-Probability-and-Statistics/tree/main/Book5_Ch20_Python_Codes

