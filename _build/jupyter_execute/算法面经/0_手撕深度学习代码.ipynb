{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手撕深度学习代码系列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制系列\n",
    "\n",
    "> 参考：https://zhuanlan.zhihu.com/p/366592542\n",
    "\n",
    "**1. Attention**\n",
    "\n",
    "输入是`query`，`key`和`value`，注意力机制首先计算`query`与每个`key`的关联性（compatibility），每个关联性作为每个`value`的权重（weight），各个权重与`value`的乘积相加得到输出。\n",
    "\n",
    "> 例如，厨房里有苹果、青菜、西红柿、玛瑙筷子、朱砂碗，每个物品都有一个key（$d_k$维向量）和value（ $d_v$维向量）。现在有一个“红色”的query（$d_q$维向量），注意力机制首先计算“红色”的query与苹果的key、青菜的key、西红柿的key、玛瑙筷子的key、朱砂碗的key的关联性，再计算得到每个物品对应的权重，最终输出 =（苹果的权重x苹果的value + 青菜的权重x青菜的value + 西红柿的权重x西红柿的value + 玛瑙筷子的权重x玛瑙筷子的value + 朱砂碗的权重x朱砂碗的value）。最终输出包含了每个物品的信息，由于苹果、西红柿的权重较大（因为与“红色”关联性更大），因此最终输出受到苹果、西红柿的value的影响更大。\n",
    "\n",
    "![Scaled Dot-Product Attention计算过程](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/11111.jpg)\n",
    "\n",
    "注意：\n",
    "+ 注意力掩码的作用是允许我们发送不同长度的批次数据一次性的发送到transformer中。在代码中是通过将所有序列填充到相同的长度。注意力掩码中，我们的输入是0和1，但是在最终的计算时，会将在将无效位置的注意力权重设置为一个很小的值，通常为负无穷（-inf），以便在计算注意力分数时将其抑制为接近零的概率。\n",
    "\n",
    "![y9pRHt](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/y9pRHt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 4])\n",
      "torch.Size([2, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "# 1.Scaled Dot-Product Attention from \"Attention is all you need\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q: [b, c_q, d_q], k: [b, c_q, d_k], v: [b, c_q, d_v]\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) / self.scale  # 1&2. Matmul and scale -> [b, c_q, c_k]\n",
    "        \n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3.mask\n",
    "        attn = self.softmax(u) # 4\n",
    "        output = torch.bmm(attn, v) # 5.output [b, c_q, c_k]*[b, c_v, d_v] -> [b, c_q, d_v]\n",
    "        \n",
    "        return attn, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q, d_k, d_v = 128, 128, 64\n",
    "    batch = 2\n",
    "\n",
    "    q = torch.randn(batch, n_q, d_q)\n",
    "    k = torch.randn(batch, n_k, d_k)\n",
    "    v = torch.randn(batch, n_v, d_v)\n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "    attn, output = attention(q, k, v, mask=mask)\n",
    "\n",
    "    print(attn.shape)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知识点：\n",
    "+ `torch.bmm`: batch matrix multiplication, `[b, n ,d]`和`[b, d, p]`两个矩阵相乘\n",
    "+ `u.masked_fill(mask, value)`: 对tensor将mask中为True的元素对应的基础Tensor的元素设置为值value。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. multi-head attention**\n",
    "\n",
    "只求一次注意力的过程可以叫做单头注意力。多头注意力就是对同样的Q, K, V求多次注意力，得到多个不同的output，再把这些不同的output连接起来得到最终的output。\n",
    "**多头注意允许模型在不同位置共同注意来自不同表示子空间的信息**。不同头部的output就是从不同层面（representation subspace）考虑关联性而得到的输出。\n",
    "\n",
    "> 例如，以“红色”为query，第一个头部（从食物层面考虑）得到的output受到苹果、西红柿的value的影响更大；第二个头部（从餐具层面考虑）得到的output受到玛瑙筷子、朱砂碗的value的影响更大。相比单头注意力，多头注意力可以考虑到更多层面的信息。\n",
    "\n",
    "![mnI5Bk](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/mnI5Bk.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 4])\n",
      "torch.Size([2, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "\n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "\n",
    "        q = self.fc_q(q) # 1.单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出\n",
    "\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
    "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_q, n_k, n_v = 2, 4, 4\n",
    "    d_q_, d_k_, d_v_ = 128, 128, 64\n",
    "\n",
    "    q = torch.randn(batch, n_q, d_q_)\n",
    "    k = torch.randn(batch, n_k, d_k_)\n",
    "    v = torch.randn(batch, n_v, d_v_)    \n",
    "    mask = torch.zeros(batch, n_q, n_k).bool()\n",
    "\n",
    "    mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
    "    attn, output = mha(q, k, v, mask=mask)\n",
    "\n",
    "    print(attn.size())\n",
    "    print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "+ 多头注意力主要是多了linear层，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. self-attention**\n",
    "\n",
    "当注意力的query和key、value全部来自于同一堆东西时，就称为自注意力。如下图所示，query和key、value全都来源于X。\n",
    "\n",
    "> 举个例子：厨房里有苹果、青菜、西红柿、玛瑙筷子、朱砂碗，每个物品都会计算得到一个query，以及一个key和value。“苹果”的query与苹果、青菜、西红柿、玛瑙筷子、朱砂碗的key和value做注意力，得到最终输出。其他物品的query也如此操作。这样，输入5个物品，有5个query，得到5个输出，相当于将这5个物品换了一种表示形式，而这新的表示形式（得到的输出）每个都是是考虑了所有物品的信息的。\n",
    "\n",
    "自注意力通过X求query和key、value的计算过程如下图所示：\n",
    "\n",
    "![568hTZ](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/568hTZ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 4])\n",
      "torch.Size([2, 4, 80])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_q))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "        self.init_parameters()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_head, d_k, d_v, d_k, d_v, d_o)\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.xavier_uniform_(param)\n",
    "            \n",
    "    def forward(self, x, mask=None):\n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        \n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        return attn, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_x = 4\n",
    "    d_x = 80\n",
    "\n",
    "    x = torch.randn(batch, n_x, d_x)\n",
    "    mask = torch.zeros(batch, n_x, n_x).bool()\n",
    "\n",
    "    selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)\n",
    "    attn, output = selfattn(x, mask=mask)\n",
    "\n",
    "    print(attn.size())\n",
    "    print(output.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.关于mask的设置**\n",
    "对于self-attention，其mask如下：\n",
    "\n",
    "![ALQi0b](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/ALQi0b.png)\n",
    "\n",
    "![97z24v](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/97z24v.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D卷积实现\n",
    "\n",
    "> 参考：https://www.zhihu.com/tardis/bd/art/349683405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, kernel, bias, stride=None, pad=None):\n",
    "    n, c, h, w = x.shape\n",
    "    d, c, k, j = kernel.shape\n",
    "    \n",
    "    # padding\n",
    "    x_pad = torch.zeros(n, c, h+2*padding, w+2*padding)\n",
    "    if pad is not None and pad > 0:\n",
    "        x_pad[:, :, pad:-pad, pad:-pad] = x\n",
    "    else:\n",
    "        x_pad = x\n",
    "        \n",
    "    # conv\n",
    "    x_pad = x_pad.unfold(2, k, stride)  # [n, c, h, w] -> [n, c, (h+2*padding-k*2//2)//stride, h, j]\n",
    "    x_pad = x_pad.unfold(3, j, stride)  # [n, c, (h+2*padding-k*2//2)//stride, h, j] -> [n, c, (h+2*padding-k*2//2)//stride, (h+2*padding-k*2//2)//stride, k, j]\n",
    "    out = torch.einsum(                          # 按照滑动窗相乘，\n",
    "        'nchwkj,dckj->ndhw',                    # 并将所有输入通道卷积结果累加 [nchwkj,dckj]->ndhw 表示在k,j维度上进行点乘，注意h,w和我们前面定义的h,w不一样\n",
    "        x_pad, weight)\n",
    "    out = out + bias.view(1, -1, 1, 1)          # 添加通道维度偏置值\n",
    "    return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：\n",
    "1. `Einsum`：在Einsum中，箭头从左边到右边消失了什么参数，那公式前就加一个带什么参数的求和符。本案例中消失了k，因此我们需要在加上对带k的求和符，转化为数学公式如下：\n",
    "\n",
    "![kD99VS](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/kD99VS.png)\n",
    "\n",
    "![Uqx74s](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Uqx74s.png)\n",
    "\n",
    "2. `torch.unfold()`将原始张量进行分片，也就是分成每个kernel乘的对应位置的大小的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python实现BN批量归一化\n",
    "\n",
    "> 参考\n",
    "> + 代码: https://zhuanlan.zhihu.com/p/100672008\n",
    "> + 原理分析: https://zhuanlan.zhihu.com/p/609131550, https://www.cnblogs.com/huwj/p/10765114.html\n",
    "\n",
    "![gK8Q6I](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/gK8Q6I.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    def batch_norm(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class MyBN:\n",
    "    def __init__(self, momentum=0.9, eps, n_features):\n",
    "        self._momentum = momentum\n",
    "        self._eps = eps\n",
    "        self._running_mean = 0\n",
    "        self._running_var = 1\n",
    "        self._beta = np.zeros(shape=(n_features,))\n",
    "        self._gamma = np.ones(shape=(n_features,))\n",
    "        \n",
    "   def batch_norm(self, x):\n",
    "        mu = x.mean(axis=0)\n",
    "        var = v.var(axis=0)\n",
    "        self.runing_mean = (1-self._momentum)*mu + self._momentum*self._running_mean\n",
    "        self.runing_var = (1-self._momentum)*var + self._momentum*self._running_var\n",
    "        x_hat = (x - mu) / np.sqrt(var + self._eps)\n",
    "        y = self._gamma * x_hat + self._beta\n",
    "        \n",
    "       return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "> 参考： https://blog.csdn.net/liangjiu2009/article/details/107352164\n",
    "\n",
    "![s6Ndmh](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/s6Ndmh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    # shape: b, ...\n",
    "    b = pred.shape[0]\n",
    "    pred = pred.view(b, -1)\n",
    "    target = target.view(b, -1)\n",
    "    \n",
    "    intersection = (pred * target).sum(1)\n",
    "    dice = (2 * intersection + smooth) / (pred.sum(1) + target.sum(1) + smooth)\n",
    "    \n",
    "    return (1 - dice).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 使用torch.utils.data.Dataset类来构建自定义的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param root_dir: 包含图像文件的根目录。\n",
    "        :param transform: 应用于图像的可选变换。\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 遍历目录，收集图像路径和标签\n",
    "        for filename in os.listdir(root_dir):\n",
    "            if filename.endswith('.jpg'):  # 假设图像文件后缀为.jpg\n",
    "                label = filename.split('_')[0]  # 从文件名中提取标签\n",
    "                image_path = os.path.join(root_dir, filename)\n",
    "                self.images.append(image_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中的图像数量。\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引获取一个图像和它的标签。\n",
    "        \"\"\"\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # 确保图像是RGB格式\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# 创建数据集的变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 调整图像大小\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = CustomDataset(root_dir='path_to_your_dataset', transform=transform)\n",
    "\n",
    "# 现在可以使用PyTorch的DataLoader来加载数据集\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}