#! https://zhuanlan.zhihu.com/p/677154173

[toc]



# 扩散模型 | 2.SDE精讲

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/9Y7UGs.png)

在前面的博客中我们讲了score-based model的开山之作：发表于2019年NIPS的**Generative Modeling by Estimating Gradients of the** **Data Distribution**，也称作*Noise Conditional Score Network (NCSN)*。

https://zhuanlan.zhihu.com/p/677141632

本文主要讲解score-based model的后续工作，发表于2021年ICLR的**Score-Based Generative Modeling through Stochastic Differential Equations**，也称作*SDE*。**本文将NSCN中有限次数的加噪推广到无穷次**（连续的情况下），那么就可以得到更加一般的扩散过程，并且这个过程可以用一个随机微分方程（SDE）来表示。本文将score-based model和DDPM进行大一统。虽然SDE构造扩散模型是一个全新的框架，但是其本质还是score。

> Paper: https://arxiv.org/abs/2011.13456
>
> Code: https://github.com/yang-song/score_sde

## 摘要

本文主要介绍了一种基于随机微分方程（SDE）的得分模型生成建模框架。通过将得分模型与SDE相结合，可以实现更好的样本生成、准确的似然计算、编码操控和条件生成等功能。同时，文章还提出了一种反向扩散采样方法，可以有效地从给定的数据和模型中生成样本。该方法在图像修复、图像插值等领域具有广泛的应用前景。

## 为什么要用SDE？

在diffusion过程中，对于$t$时间点的图像$x_t$，我们考虑两种情况：

1. 当$t$固定时，$x_t$是随机变量，即$x_t \sim N(\sqrt{\bar{\alpha_t}}x_0,(1-\bar{\alpha_t})I)$ ；
2. 当$x$固定时，$x_t$是一个采样轨迹：$x_T,x_{T-1}, \cdots, x_1,x_0$。 因此$x_t$实质上是一个随机过程。**SDE是描述随机过程的工具之一。**

考虑将diffusion离散的连续过程离散化：对于DIffusion过程，加噪和去噪过程可以表示为
$$
x_0\rightarrow x_1 \rightarrow \cdots \rightarrow x_T \rightarrow x_{T-1} \rightarrow \cdots\rightarrow x_T, t \in \{0,1,\cdots,T\}
$$
在连续过程中，我们设$t\in [0,1]$，上述离散的前向加噪过程连续化，可以表示为
$$
x_t \rightarrow x_{t+\Delta t}, \Delta t\rightarrow0
$$
反向去噪过程表示为
$$
x_{t+\Delta t} \rightarrow x_t, \Delta t\rightarrow0
$$

## SDE-based Diffusion Process

![使用连续时间随机过程扰动数据增加噪声](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/denoise_vp.gif)

对于图像$x$，我们定义连续的微分过程（扩散过程）为
$$
dx = \underbrace{f(x,t)dt}_{确定性} + \underbrace{g(t)dw}_{不确定性}
$$
其中，

- $f(x,t)$：drift coefficience（漂移系数）
- $g(t)$：diffusion coefficient （diffusion系数）
- $w$： Brownian motion （布朗运动）

也就是说一次扩散$dx$是由确定性和不确定性的两个过程组合而成。布朗运动具有增量独立性、增量服从高斯分布且轨迹连续。我们定义f(x,t)dt为确定性变化过程，g(t)dw为不确定性变化过程(dw-布朗运动的微分就相当于随机采样)，如下图所示：

![连续扩散过程](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/i4zuTj.png)



> 在实际建模过程中，可以设置各种各样的SDE样式。

## SDE-based Reconstruction/Sampling/Inference Process


![通过反转扰动过程从噪声生成数据](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/denoise_vp.gif)

**Defination.** Reversing the SDE for sample generation
$$
\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g(t)^2\nabla_\mathbf{x}\log p_t(\mathbf{x})]\mathrm{d}t+g(t)\mathrm{d}\mathbf{\bar{w}},
$$
上式中，$\mathbf{\bar{w}}$表示Brownian motion in the reverse time direction，$dt$表示infinitesimal negative time step。

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/w1uJNO.png)

在我们已知前向过程的漂移、扩散系数以及$t$时刻的分数时，可以通过数值求解反向SDE，求出$\mathbf x_0$。

## 通过denoising score matching估计反向SDE

我们可以使用时间相关的分数函数去利用数值解法求解反向SDE。我们使用下述weighted sum of denoising scoring matching目标函数，训练时间相关的score-based model $\mathbf s_\theta(\mathbf x, t)$：
$$
\begin{align}

\min_\theta \mathbb{E}_{t\sim \mathcal{U}(0, T)} [\lambda(t) \mathbb{E}_{\mathbf{x}(0) \sim p_0(\mathbf{x})}\mathbf{E}_{\mathbf{x}(t) \sim p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))}[ \|s_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))\|_2^2]],

\end{align}
$$
where $\mathcal{U}(0,T)$ is a uniform distribution over $[0, T]$ （实际情况下，$t$要做归一化，即是从$\mathcal{U}(0,1)$采样的浮点数）。$\mathbf x_0$为训练样本，$\mathbf x_t$取决于SDE的定义。 $p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))$ denotes the transition probability from $\mathbf{x}(0)$ to $\mathbf{x}(t)$, which is efficient when the draft coefficient $\mathbf f(\mathbf x,t)$  is affine （每个线性变换一定是仿射变换）。 $\lambda(t) \in \mathbb{R}_{>0}$ denotes a positive weighting function，一般设置为$\frac{1}{\mathbb{E}[\|\nabla*_{\mathbf{x}}\log p_*{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) \|_2^2]}$（特别的，当噪声分布为Gaussian，则$\lambda(t)$为Gaussian的方差），为了平衡不同时间的score matching loss的量级（保证量级相同）。

---

例如，对于加噪的图像$q_\sigma(\tilde{\mathbf{x}}\mid\mathbf{x})=\mathcal{N}(\tilde{\mathbf{x}}\mid\mathbf{x},\sigma^2I)$，其对数密度梯度为$\nabla_{\tilde{\mathbf{x}}}\operatorname{log}q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})=-(\tilde{\mathbf{x}}-\mathbf{x})/\sigma^{2}$（注意是对$\tilde{\mathbf{x}}$求导），给定一个噪声水平$\sigma$，，denoiser score matching的目标函数为：
$$
\ell(\boldsymbol{\theta};\sigma)\triangleq\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(\mathbf{x})}\mathbb{E}_{\tilde{\mathbf{x}}\sim\mathcal{N}(\mathbf{x},\sigma^2I)}\left[\left\|\mathbf{s}_{\boldsymbol{\theta}}(\tilde{\mathbf{x}},\sigma)+\frac{\tilde{\mathbf{x}}-\mathbf{x}}{\sigma^2}\right\|_2^2\right]
$$
那么，对于一系列噪声水平$\sigma \in \{\sigma_i\}_{i=1}^L$， 我们可以得到一个统一的目标函数：
$$
\mathcal{L}(\boldsymbol{\theta};\{|\sigma_i\}_{i=1}^L)\triangleq\frac{1}{L}\sum_{i=1}^{L}\lambda(\sigma_i)\ell(\boldsymbol{\theta};\sigma_i),
$$
其中，$\lambda(\sigma_i)>0$是和$\sigma_i$有关的系数，为了平衡损失，使得每个加权项数量级范围接近。关于$\lambda(\sigma_i)>0$的选择，文章中给定了一种方案，即$\lambda(\sigma)=\sigma^2$：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240110212822548.png)

那么总的训练loss便可以化简为：
$$
\mathcal{L}(\boldsymbol{\theta};\{|\sigma_i\}_{i=1}^L)\triangleq\frac{1}{2L}\sum_{i=1}^{L}||\sigma\mathbf s_\theta(\mathbf{\tilde{x}},t) + \tilde{ \mathbf{x}}-\mathbf x||_2^2,\  where\  \tilde{ \mathbf{x}}-\mathbf x\ is\ sampling\ noise.
$$


## Tips of designing scoring based models

在设计time-dependent score-based models时候，网络架构没有太多限制，只要保证输出输出维度相同，并把时间信息作为条件输入网络即可。下面是一些有用的模型设计tips：

+ 常用**U-Net**作为score network作为分数网络。
+ 可以通过**高斯随机特征的方法把时间信息注入网络中**。Specifically, we first sample $\omega \sim \mathcal{N}(\mathbf{0}, s^2\mathbf{I})$ which is subsequently fixed for the model (i.e., not learnable). For a time step $t$, the corresponding Gaussian random feature is defined as $[\sin(2\pi \omega t) ; \cos(2\pi \omega t)],$where $[\vec{a} ; \vec{b}]$ denotes the concatenation of vector $\vec{a}$ and $\vec{b}$. This Gaussian random feature can be used as an encoding for time step $t$ so that the score network can condition on $t$ by incorporating this encoding.
+ **可以rescale U-Net的输出，即乘以一个系数** $1/\sqrt{\mathbb{E}[\|\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) \|_2^2]}$。 This is because the optimal $s_\theta(\mathbf{x}(t), t)$ has an $\ell_2$-norm close to $\mathbb{E}[\|\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))]\|_2$, and the rescaling helps capture the norm of the true score. Recall that the training objective contains sums of the form $\mathbf{E}_{\mathbf{x}(t) \sim p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))}[ \|s_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))\|_2^2].$  Therefore, it is natural to expect that the optimal score model $s_\theta(\mathbf{x}, t) \approx \nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))$. (也就是说在保证loss最小的情况下，也保证估计的分数的范数也接近真实的范数)
+ Use [exponential moving average](https://discuss.pytorch.org/t/how-to-apply-exponential-moving-average-decay-for-variables/10856/3) (EMA) of weights when sampling. This can greatly **improve sample quality**, but requires slightly longer training time, and requires more work in implementation. We do not include this in this tutorial, but highly recommend it when you employ score-based generative modeling to tackle more challenging real problems.

## DDPM和score-based model统一

### VE-SDE(NCSN / Score-based model)和VP-SDE（DDPM）

到此为止，我们得到了SDE-based方法的加噪和去噪过程分别为：
$$
\mathrm{d}\mathbf{x}=\mathbf{f}(\mathbf{x},t)\mathrm{d}t+g(t)\mathrm{d}\mathbf{w},\\
\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g(t)^2\nabla_\mathbf{x}\log p_t(\mathbf{x})]\mathrm{d}t+g(t)\mathrm{d}\mathbf{\bar{w}},
$$

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/w1uJNO.png)

扩散模型可以大致分为两类：VE-SDE和VP-SDE。

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/GJJCu8.png)

由于$x_T$要尽可能接近噪声，因此要求VE-SDE的$\sigma_T$尽可能大，因此称为方差爆炸，而VP-SDE对方差进行保留，即$\sqrt{1-\bar{\alpha_t}}$约等于1。如下图：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/JEHXxs.png)

### VE-SDE、VP-SDE和SDE的前向/扩散过程的关系

SDE前向过程的离散化表示为

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/daaIu0.png)

1. VE-SDE：$\mathrm{d}\mathbf{x}=\sqrt{\frac{\mathrm{d}\left[\sigma^2(t)\right]}{\mathrm{d}t}\mathrm{d}\mathbf{w}}$， 推导如下

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240111004020022.png)

2. VP-SDE：$\mathrm{d}\mathbf{x}=-\frac{1}{2}\beta(t)\mathbf{x}\mathrm{d}t+\sqrt{\beta(t)}\mathrm{d}\mathbf{w}.$推导如下：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240111004646283.png)

### DDPM denoiser 和score estimator的关系

根据DDPM前向过程中$x_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 +  \sqrt{1 - \bar{\alpha}_t}\epsilon$，可知DDPM的denoiser近似
$$
\begin{cases}
score\ estimator \ s_\theta(x_t,t)\\
DDPM\ denoiser\ \epsilon (x_t,t) \approx \frac{x_t-\sqrt{\bar\alpha_t}x_0}{\sqrt{1-\bar\alpha_t}}
\end{cases}
$$
在DDPM中，
$$
\begin{align}
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})\\
&\propto \exp \Big(-\frac{\|\mathbf{x}_t - \sqrt{\bar\alpha_t} \mathbf{x}_{0}\|^2_2}{2(1-\bar\alpha_t)} \Big)
\end{align}
$$
那么可以求得当前$x_t$的对数密度梯度为
$$
\nabla_\mathbf{x} \log p(\mathbf{x}) = \frac{x_t - \sqrt{\bar{\alpha}_{t}}\mathbf{x}_0}{1 - \bar{\alpha}_t} 
$$
在score-based model中，有$\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p(\mathbf{x})$。当我们有一个$\epsilon_\theta (x_t,t) $，可以得到结论，即
$$
\mathbf{s}_\theta(\mathbf{x}) = \frac{1}{\sqrt{1-\bar\alpha_t}}\epsilon (x_t,t)
$$
上式展示了DDPM denoiser 和score estimator的关系的关系。因此，这意味着，**如果我现在有一个训练好的DDPM，那么其加一个系数就是Score estimator**。‼️这是一个非常巧合且非常棒的结论，从此打破了两种模型之间的gap，为未来的各种预训练diffusion model和各种加速采样方式奠定了坚实的基础。

## 代码实战：MINST数据训练一个score-based model

在colab实现了一个生成MNIST数据的score-based model（大概训练半小时），并测试了不同的采样方式，如：**Euler_SDE，PC_sampling和ODE**，结果表明：**效果上：PC>SDE>ODE，时间成本上：ODE<SDE<PC.**

**Project链接**：https://drive.google.com/file/d/1OYunpicIYf1Jf0JxLDcVXc0GRtk5uw3X/view?usp=sharing

## 补充1：逆向SDE的推导

**推导**：

这一小节我们**推导SDE-based的反向重建过程**。对于连续扩散模型：$dx = \underbrace{f(x,t)dt}_{确定性} + \underbrace{g(t)dw}_{不确定性}, x_t\rightarrow x_{t+\Delta t}$，令$dw=\sqrt{\Delta t}\varepsilon$，我们先离散化扩散公式：
$$
x_{t+\Delta t}-x_{t}=f(x_{t},t)\Delta t+g(t)\underline{\sqrt{\Delta t}}\varepsilon, \varepsilon\sim\mathcal{N}(0,I) \\
x_{t+\Delta t}=x_{t}+f(x_{t},t)\Delta t+g(t)\underline{\sqrt{\Delta t}}\varepsilon, \varepsilon\sim\mathcal{N}(0,I)\\
p(x_{t+\Delta t}|x_t)\sim \mathcal N (x_{t}+f(x_{t},t)\Delta t, g^2(t)\Delta t I)
$$

重建（$x_{t+\Delta t}\rightarrow x_t$）公式可由贝叶斯公式得到：

$$
\begin{align}
p(x_t|x_{t+\Delta t}) &= \frac{p(x_{t+\Delta t}|x_t)P(x_t)}{px_{t+\Delta t}}\\
&=p(x_{t+\Delta t}|x_t)\mathrm{exp}(\mathrm{log} p(x_t)-\mathrm{log}p(x_{t+\Delta t}))
\end{align}
$$

> 写为指数的形式为了方便最后凑出score的形式。

对$\mathrm{log}p(x_{t+\Delta t}))$作一阶泰勒展开，可以化简为：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240110235351096.png)

实际上，$logp(x_{t+\Delta_t})$为关于x和t的函数，因此一阶导有两项。代入$p(x_{t+\Delta t}|x_t)$并进行配方：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240110235914773.png)

当$\Delta t \rightarrow0$，对上式进行近似（含$\Delta_t$一次项为0）。因为我们是$x_{t+\Delta t}\rightarrow x_t$的过程，我们希望从推导的高斯分布提取均值和方差，我们希望均值和方差由$t+\Delta t$提供，通过更换变量名。近似结果为：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/xbbrdr.png)

那么可以得到每一步采样结果$p(x_t|x_{t+\Delta t })$的均值和方差为：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/iQnfBA.png)

将上式写成连续形式，即**SDE的采样过程为：**
$$
\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g(t)^2\nabla_\mathbf{x}\log p_t(\mathbf{x})]\mathrm{d}t+g(t)\mathrm{d}\mathbf{\bar{w}},
$$
实际实验中，离散的采样过程为

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240111001557852.png)

可以发现，离散化后的采样公式中存在我们需要估计的score。

## 补充2：VE和VP等价性证明

通过一些简单的参数变换，我们可以证明出VE-SDE和VP-SDE是等价的：

![](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/8tMPXi.png)

## 总结

本文使用SDE模型将score-based model和DDPM进行大一统。

+ 如果现在有一个训练好的DDPM，那么其加一个系数就是Score estimator。

+ 通过一些简单的参数变换，我们可以证明出**VE-SDE和VP-SDE是等价的**。

存在的挑战

+ 采样速度慢：ODE求解加速，隐空间diffusion，...
+ 如何处理离散数据分布（分数是定义在连续分布上）：构建autoencoder，将离散数据分布映射到连续空间上



## 参考

+ [blog | Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)
+ [Song Y, Sohl-Dickstein J, Kingma D P, et al. Score-Based Generative Modeling through Stochastic Differential Equations[C]//International Conference on Learning Representations. 2020.](https://arxiv.org/abs/2011.13456)
+ [【扩散模型 Diffusion Model 3-2 SDE（一）】](https://www.bilibili.com/video/BV19M411z7hS?vd_source=225dba48b31d269151658db856705273)
+ [【扩散模型 Diffusion Model 3-3 SDE（二）】](https://www.bilibili.com/video/BV1814y1n7Eh?vd_source=225dba48b31d269151658db856705273)
+ [【扩散模型 Diffusion Model 3-4 SDE（三）】](https://www.bilibili.com/video/BV1GY411d7Gy?vd_source=225dba48b31d269151658db856705273)
+ https://blog.csdn.net/qq_43800752/article/details/129422654
+ [62、Score Diffusion Model分数扩散模型理论与完整PyTorch代码详细解读](https://www.bilibili.com/video/BV1Dd4y1A7oz?vd_source=225dba48b31d269151658db856705273)

