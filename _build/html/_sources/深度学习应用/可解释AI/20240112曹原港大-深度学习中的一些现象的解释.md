#! https://zhuanlan.zhihu.com/p/677445352

# 【深度学习中的一些有趣现象及其理论解释】——曹原（香港大学）-2024

> Talk：【【深度学习中的一些有趣现象及其理论解释】——曹原（香港大学）】https://www.bilibili.com/video/BV11K4y167cc?vd_source=225dba48b31d269151658db856705273

![rx2khS](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/rx2khS.png)

## 背景介绍

![r9KTWm](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/r9KTWm.png)

![image-20240112105934410](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/image-20240112105934410.png)

例如热力学中的一些规律可以通过微观分析的规律体现：

![8Iyofj](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/8Iyofj.png)

深度学习中的一些宏观上的观察也可以使用一些变量去衡量：，例如：损失、网络宽度、信噪比等

![Cxw57F](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Cxw57F.png)

## 良性过拟合

### 问题简介

通常来说，神经网络是过参数化模型，过参数化模型意味着模型具有非常多的参数，这会导致模型在训练集上表现很好，但在测试集上表现很差。但是，过参数化模型并不是所有情况都会导致过拟合，例如下图在ResNet18中，随着模型参数的增加，训练error减小，test error也在减小。

![Q1Jirv](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Q1Jirv.png)

**良性过拟合有时候会和恶性过拟合相互转换**在MNIST边缘加上噪声，test loss会随着噪声的增加而增加。后期，网络可能会更加依赖noise进行分类：

![QaUl3h](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/QaUl3h.png)

### 具体分析

![mpQAi5](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/mpQAi5.png)

![VYtAyv](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/VYtAyv.png)

考虑相应两层卷积神经网络：图片通过2m个卷积核和激活函数，再通过一个线性层使得结果在±1/m。

![CSWWwZ](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/CSWWwZ.png)

![hJMxtx](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/hJMxtx.png)

可以得到两个几乎互补的条件使得测试结果完全不同

![5VIES8](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/5VIES8.png)


![b9Mzvv](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/b9Mzvv.png)

![l4VvTO](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/l4VvTO.png)


## Adam和SGD的泛化gap

Adam一般收敛更快，但是test accuracy可能SGD更好。原因在于实际使用中神经网络function class过于复杂。有很多function可以再训练集上达到100%，然而在test上表现可能不佳。

![Pa0Uhe](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Pa0Uhe.png)

![RMxPOR](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/RMxPOR.png)


**Implicit bias：**当模型过参数化，会有无穷个模型是training data的解。但是我们最终只能得到一个解。下面举例SDG在求解线性logistic regression的结论：最终会收敛到和SVD一样的maximum margin classifier。

![TE9J7m](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/TE9J7m.png)

具体分析

![HZd64U](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/HZd64U.png)

![ZONaxy](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/ZONaxy.png)

![sQmklr](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/sQmklr.png)

## Implicit bias with batch normalization

Batch norm的引入会使得margin variance $y_i f(x_i)$稳定。

![KIY45y](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/KIY45y.png)

![jH64pZ](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/jH64pZ.png)

具体分析：

![sdgjTv](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/sdgjTv.png)

BatchNorm可以加速收敛

![zIovFn](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/zIovFn.png)

![Wi6Xgs](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Wi6Xgs.png)

## 总结

![g7LzmR](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/g7LzmR.png)













