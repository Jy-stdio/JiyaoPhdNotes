# 概率图模型入门

> from: 【概率图模型初步：从贝叶斯网络到HMM模型】https://www.bilibili.com/video/BV1E4411a7zY?vd_source=225dba48b31d269151658db856705273

## 概率基础 

![lUID1D](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/lUID1D.png)

![TF3MGP](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/TF3MGP.png)

![8HrQsS](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/8HrQsS.png)

![zfS8FR](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/zfS8FR.png)

## Bayesian Network



### 概率图模型的动机及举例

> + 知道了联合分布就能进一步求出边缘分布或者条件分布；  
> + 用一种有效的方式对联合分布进行建模;

![vaDreW](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/vaDreW.png)

**举个例子：**

对于两个选项的选课问题，有三个人，我们想要得到三个人是否选三门课的联合分布。可能的组合有$2^3$种：

![vDInB9](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/vDInB9.png)。

当有成千上万个变量的时候，对每种可能性组合进行保存就会造成维度灾难。

----

所以，我们可以假设这几种变量存在一定的依赖关系，以一种图的形式对变量的关系进行建模，便可以减少储存联合分布的空间：

![Syt7S4](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Syt7S4.png)

举个例子，假设有5个变量：课程难度，GPA，IQ，GRE，老师是否愿意写推荐信，我们可以建模这几个变量的依赖关系：

![ImPTZk](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/ImPTZk.png)

**概率图模型的结论是：如果我们知道这几个变量的联合概率等于每个节点given parients的节点概率的乘积。**这样的好处是联合分布的复杂度以指数程度降低。

![VIAIiL](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/VIAIiL.png)

如下图：联合分布存储的复杂度从左边的15降到右边的9次。

![IP7zGN](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/IP7zGN.png)

**一般来说，图的关系是由专家定义出来的。**

---

得到概率图及联合概率后，我们就可以使用概率图进行推断。想要高效实现，这就涉及动态规划，不想高效实现推断，那动态规划也不一定必要。下面是最暴力的推断方法，也就是上帝视角来看推断我们要干的事情。

![hMslbL](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/hMslbL.png)

但是一般来说完整的联合分布没法全部得到，我们可以拆成多个概率，然后对每一项暴力求解：

![KBEHfJ](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/KBEHfJ.png)

但是这样因噎废食，我们降低了存储的复杂度，但是增加了计算的复杂度！！

---
一种更优雅的方式动态规划：

> 机器学习中，一般只在：概率图模型（维特比算法）、强化学习中用到（求解马尔科夫决策过程）；

我们再上述计算过程中，某些**变量其实计算了好几遍，我们可以消除这些变量的重复计算。** 也就是说，我们先把求和公式中出现单独的变量拿出来先求和，比如先求$\sum p(w|s=1,R)$，其实这一项等于1就消掉了。最后只剩下了蓝色部分的两项求和。

![XRrlIT](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/XRrlIT.png)

---
但是这样动态规划的技巧算出来的是一个精确值，是成本比较高的，我们可以使用Gibbs等采样方式，去逼近这个推断的精确值。

![6xcRmL](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/6xcRmL.png)

## 概率图应用一：朴素贝叶斯

朴素贝叶斯的本质就是简单的概率图模型。比如垃圾邮件分类任务，可以建立如下概率图，邮件分类可以通过如下几个关键词的出现进行分类。所谓naive就是联合分布能写成这样简单的概率乘积。

![Kt3dvF](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Kt3dvF.png)

假设每个词的出现是条件独立，那么后验分布的计算公式如下（贝叶斯公式）：

![hnMzt5](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/hnMzt5.png)

## 概率图应用二：Markov model

![kWhyUY](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/kWhyUY.png)

马尔科夫模型中，我们只需要知道初始值和状态转移概率（共享），那么联合分布就知道了。

----
举个例子：我们想求上图的$P(x_1, x_2, x_3, x_4)$，那么暴力方法是：

![kAOfIX](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/kAOfIX.png)

加快计算：变量消元法/动态规划

![XGgnrb](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/XGgnrb.png)

----
再举个例子：手算一下$P(x_2)$
![xJ3ZxC](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/xJ3ZxC.png)
![TLkz7Y](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/TLkz7Y.png)

> 上帝视角就是我们知道联合分布，我们就能干所有事情！！！

上面例子写成矩阵形式

![CpYfe4](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/CpYfe4.png)

$x_\infty$与初始状态无关，即无记忆性。

> 证明：马尔科夫链无记忆性
>对于矩阵，$AX = \lambda X$，则有$A^2X = \lambda AX = \lambda \cdot \lambda X = \lambda^2 X$，则$A^kX = \lambda^k X$。
>
> 那么对于马尔科夫链，$X_k = A\cdot ... A\cdot AX_0 = A^kX_0$。假设A有n个互相独立的特征向量$[C_1,...,C_n]$（一般来说马尔科夫链是非病态的，也就是都是稳态的），那么$X_0$可以表示为$x_0 = a_1C_1 + ... +a_kC_k$，也就是特征向量的线性组合。
>
>则$X_k = A^kX_0 = a_1A^k\cdot C_1 + a_kA^k\cdot C_k = a_1\lambda_1^k\cdot C_1 + a_k\lambda_k^k\cdot C_k$
>
>claim：
> + 马尔科夫矩阵有一个特征值为1，其余都小于1(也就是说，稳态的概率为特征值为1的特征向量的倍数)
>
> ![wZQT9d](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/wZQT9d.png)

![PnjFeM](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/PnjFeM.png)

## 概率图应用三：HMM

![t4yplr](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/t4yplr.png)

![LGAOGy](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/LGAOGy.png)

![LRbQor](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/LRbQor.png)

![Ofc1Fp](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/Ofc1Fp.png)

![CIi2tP](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/CIi2tP.png)

![AvOgWU](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/AvOgWU.png)

![vTaQp3](https://ossjiyaoliu.oss-cn-beijing.aliyuncs.com/uPic/vTaQp3.png)


> 马尔科夫模型要注意的点：
> 
> 1. 马尔科夫模型不适合处理大规模的数据；
>
> 2. LSTM可以做的任务，HMM都可以做，HMM适合小数据，效果可能好，大数据还是LSTM效果更好；
